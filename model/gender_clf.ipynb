{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import normalize\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn import tree\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import svm\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, classification_report, f1_score, recall_score, precision_score\n",
    "from sklearn.ensemble import ExtraTreesClassifier, RandomForestClassifier\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.feature_selection import f_classif, mutual_info_classif, SelectKBest, RFE, VarianceThreshold\n",
    "from imblearn.under_sampling import RandomUnderSampler, ClusterCentroids\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import pandas as pd \n",
    "import numpy as np \n",
    "from numpy import inf\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "from collections import Counter\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_kbest(data_frame, target, linear_rel=True, k=4):\n",
    "    \"\"\"\n",
    "    Selecting K-Best features for classification\n",
    "    :param data_frame: A pandas dataFrame with the training data\n",
    "    :param target: target variable name in DataFrame\n",
    "    :param k: desired number of features from the data\n",
    "    :returns feature_scores: scores for each feature in the data as \n",
    "    pandas DataFrame\n",
    "    \"\"\"\n",
    "    scores = []\n",
    "    if linear_rel == True: \n",
    "        feat_selector = SelectKBest(f_classif, k=k)\n",
    "        _ = feat_selector.fit(data_frame.drop(target, axis=1), data_frame[target])\n",
    "        col_name = \"F Score\"\n",
    "    else:\n",
    "        feat_selector = SelectKBest(mutual_info_classif, k=k)\n",
    "        _ = feat_selector.fit(data_frame.drop(target, axis=1), data_frame[target])\n",
    "        col_name = \"Mutual Information\"\n",
    "    \n",
    "    feat_scores = pd.DataFrame()\n",
    "    feat_scores[col_name] = feat_selector.scores_\n",
    "    feat_scores[\"P Value\"] = feat_selector.pvalues_\n",
    "    feat_scores[\"Support\"] = feat_selector.get_support()\n",
    "    feat_scores[\"Attribute\"] = data_frame.drop(target, axis=1).columns\n",
    "    \n",
    "    return feat_scores \n",
    "\n",
    "def preprocess_data_frame(data_frame, columns):\n",
    "    \"\"\"\n",
    "    Dummifying and encoding variables of dataframe, droping rows with NAN values\n",
    "    :param data_frame: A pandas DataFrame to be processed\n",
    "    pandas DataFrame\n",
    "    :param columns: List of column names of categorical variables \n",
    "    :returns data_frame: encoded variable DataFrame\n",
    "    \"\"\"\n",
    "    data_frame = data_frame.dropna()\n",
    "    data_frame = data_frame.reset_index(drop=True)\n",
    "    \n",
    "    for col in columns:\n",
    "#         print(data_frame[col])\n",
    "        data_frame[col] = encode_variable(data_frame[col])\n",
    "#         print(data_frame[col])\n",
    "    data_frame = pd.get_dummies(data_frame)\n",
    "    return data_frame\n",
    "    \n",
    "def encode_variable(series):\n",
    "    \"\"\"\n",
    "    Encoding categorical variables to numericial values\n",
    "    :param series: A pandas Series with categorical values\n",
    "    pandas DataFrame\n",
    "    :returns data_frame: encoded variable DataFrame\n",
    "    \"\"\"\n",
    "    le = sklearn.preprocessing.LabelEncoder()\n",
    "    le.fit(series)\n",
    "    print(list(le.classes_))\n",
    "    print(set(le.transform(series))) \n",
    "    data_frame = pd.DataFrame({\n",
    "        series.name: le.transform(series)\n",
    "    })\n",
    "    return data_frame\n",
    "    \n",
    "def split_dataframe(data_frame, target):\n",
    "    \"\"\"\n",
    "    Split dataframe to predictors and target\n",
    "    :param data_frame: A pandas dataFrame with the training data\n",
    "    :param target: target variable name in DataFrame\n",
    "    :returns DataFrames: X (predictors) and y (target) dataframes \n",
    "    \"\"\"\n",
    "    X = data_frame.drop([target], axis=1)\n",
    "    y = encode_variable(data_frame[target])\n",
    "    return X, y \n",
    "\n",
    "def get_features_variance_threshold(data_frame,target=\"gender\", threshold=0.2):\n",
    "    \"\"\"\n",
    "    Returns list of features with a variance above the specified threshold\n",
    "    :param data_frame: A pandas dataFrame with the training data\n",
    "    :param threshold: float value between 0 and 1 \n",
    "    :returns feature_scores: list of features\n",
    "    \"\"\"\n",
    "    if target in data_frame.columns:\n",
    "        data_frame = data_frame.drop([target], axis=1)\n",
    "    selector = VarianceThreshold(threshold=threshold)\n",
    "    selector.fit(data_frame)\n",
    "    # Get the indices of zero variance feats\n",
    "    feat_ix_keep = selector.get_support(indices=True)\n",
    "#     print(feat_ix_keep)\n",
    "    return data_frame.columns[feat_ix_keep]\n",
    "\n",
    "def get_features_univariate(data_frame, k=5, linear=True, target=\"gender\"):\n",
    "    \"\"\"\n",
    "    Returns list of features selected using the specified univariate method\n",
    "    :param data_frame: A pandas dataFrame with the training data\n",
    "    :param k: top k features to select  \n",
    "    :returns data_frame: with selected features\n",
    "    \"\"\"\n",
    "    df = show_kbest(data_frame,target=target,linear_rel=linear, k=k)\n",
    "    df = df[df[\"Support\"] == True]\n",
    "    columns = df[\"Attribute\"].values\n",
    "    return columns\n",
    "\n",
    "def get_features_rfe(data_frame,model,k=5):\n",
    "    \"\"\"\n",
    "    Returns list of features (k specified) selected using RFE for\n",
    "    :param data_frame: A pandas dataFrame with features and labels\n",
    "    :param k: top k features to select  \n",
    "    :returns list: most relevant features \n",
    "    \"\"\"\n",
    "    X = data_frame.drop(\"gender\", axis=1)\n",
    "    y = data_frame[\"gender\"]\n",
    "    selector = RFE(model, k, step=1)\n",
    "    selector = selector.fit(X, y)\n",
    "#     print(selector.support_)\n",
    "    df = pd.DataFrame({\n",
    "        \"feature\": X.columns,\n",
    "        \"support\": selector.support_\n",
    "    })\n",
    "    return list(df[df[\"support\"] == True][\"feature\"])\n",
    "\n",
    "    \n",
    "    \n",
    "def under_sample(X,y):\n",
    "    \"\"\" Returns resampled features and labels\n",
    "    :param X: Features data frame\n",
    "    :param y: Label data frame\n",
    "    :return X_resampled, y_resampled: Numpy features array and corresponding lable array\n",
    "    \"\"\"\n",
    "    rus = RandomUnderSampler(random_state=0)\n",
    "    X_resampled, y_resampled = rus.fit_sample(X, y)\n",
    "    print(sorted(Counter(y_resampled).items()))\n",
    "    return X_resampled, y_resampled\n",
    "\n",
    "def generate_classification_report(X_train, X_test, y_train, y_test, model):\n",
    "    \"\"\" Prints classification report\n",
    "    :param X: Features data frame\n",
    "    :param y: Label data frame\n",
    "    :param model: Sklearn model object\n",
    "    \"\"\"\n",
    "#     X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "#     y_test = list(np.array(y_test).flatten())\n",
    "#     y_train = list(np.array(y_train).flatten())\n",
    "    model.fit(X_train, y_train)\n",
    "    pred = model.predict(X_test)\n",
    "    target_names = ['female', 'male']\n",
    "    mean = cross_val_score(model, X_test, y_test, cv=10, scoring=\"accuracy\").mean()\n",
    "    var = cross_val_score(model, X_test, y_test, cv=10, scoring=\"accuracy\").var()\n",
    "#     print(\"Cross Validated Accuracy: mean - {}, var - {}\".format(mean, var))\n",
    "    print(\"Accuracy Score: {} Var: {}\".format(mean, var))\n",
    "    print(\"\\nConfusion Matrix:\\n\")\n",
    "    print(confusion_matrix(y_test, pred))\n",
    "    print(\"\\nClassification Report:\\n\")\n",
    "    print(classification_report(y_test, pred, target_names=target_names))\n",
    "    \n",
    "def change_of_evaluation_rfe(X_train, X_test,y_train,y_test,model, metric=\"precision\"):\n",
    "    \"\"\"\n",
    "    Plots accuracy with respect to the number of features left, and prints the names of features\n",
    "    :param X_train: A pandas dataFrame with train features\n",
    "    :param X_test: A pandas dataFrame with test features\n",
    "    :param y_train: A pandas dataFrame with train labels\n",
    "    :param y_test: A pandas dataFrame with test labels\n",
    "    :param model: Sklearn classification model object\n",
    "    :param metric: Metric to show: precision, recall, accuracy\n",
    "    :return num_features: Returns the top k features to be selected\n",
    "    \"\"\"\n",
    "    base = pd.concat([pd.concat([X_train,X_test]), pd.concat([y_train,y_test])], axis=1)\n",
    "    X = pd.concat([X_train,X_test])\n",
    "    y = pd.concat([y_train,y_test])\n",
    "    score = []\n",
    "    for k in range(len(X.columns),0,-1):\n",
    "        columns = get_features_rfe(base,model,k)\n",
    "        X_tr = X_train[columns]\n",
    "        X_te = X_test[columns]\n",
    "\n",
    "        model.fit(X_tr, y_train)\n",
    "        mean = cross_val_score(model, X_te, y_test, cv=10, scoring=metric).mean()\n",
    "        variance = cross_val_score(model, X_te, y_test, cv=10).var()\n",
    "\n",
    "        score.append([k, mean, variance])\n",
    "\n",
    "    df = pd.DataFrame(score, columns=[\"num_features\", \"mean\",\"variance\"])\n",
    "    sns.pointplot(df[\"num_features\"], df[\"mean\"])\n",
    "    plt.show()\n",
    "    return df.sort_values(\"mean\", ascending=False)[[\"num_features\"]].values[0][0]\n",
    "\n",
    "def change_of_evaluation_univariate_feature_selection(X_train, X_test, y_train, y_test,model, metric=\"precision\", linear=True):\n",
    "    \"\"\"\n",
    "    Plots accuracy with respect to the number of features left, and prints the names of features\n",
    "    :param X_train: A pandas dataFrame with train features\n",
    "    :param X_test: A pandas dataFrame with test features\n",
    "    :param y_train: A pandas dataFrame with train labels\n",
    "    :param y_test: A pandas dataFrame with test labels\n",
    "    :param linear: If true > F-score if false > mutual information\n",
    "    :param model: Sklearn classification model object\n",
    "    :param metric: Metric to show: precision, recall, accuracy\n",
    "    :return num_features: Returns the top k features to be selected\n",
    "    \"\"\"\n",
    "    X = pd.concat([X_train,X_test])\n",
    "    y = pd.concat([y_train,y_test])\n",
    "    data = pd.concat([X,y], axis=1)\n",
    "    score = []\n",
    "    for k in range(len(X.columns),0,-1):\n",
    "        df = show_kbest(data,\"gender\",linear_rel=linear, k=k)\n",
    "        df = df[df[\"Support\"] == True]\n",
    "        columns = df[\"Attribute\"].values\n",
    "        X_tr = X_train[columns]\n",
    "        X_ts = X_test[columns]\n",
    "\n",
    "        model.fit(X_tr, y_train)\n",
    "        mean = cross_val_score(model, X_ts, y_test, cv=10, scoring=metric).mean()\n",
    "        variance = cross_val_score(model, X_ts, y_test, cv=10).var()\n",
    "\n",
    "        score.append([k, mean, variance])\n",
    "\n",
    "    df = pd.DataFrame(score, columns=[\"num_features\", \"mean\",\"variance\"])\n",
    "    sns.pointplot(df[\"num_features\"], df[\"mean\"])\n",
    "    plt.show()\n",
    "    return df.sort_values(\"mean\", ascending=False)[[\"num_features\"]].values[0][0]\n",
    "\n",
    "def change_of_evaluation_variance_threshold(X_train, X_test, y_train, y_test,model,metric=\"precision\"):\n",
    "    \"\"\"\n",
    "    Plots accuracy with respect to the number of features left, and prints the names of features\n",
    "    :param X_train: A pandas dataFrame with train features\n",
    "    :param X_test: A pandas dataFrame with test features\n",
    "    :param y_train: A pandas dataFrame with train labels\n",
    "    :param y_test: A pandas dataFrame with test labels\n",
    "    :param model: Sklearn classification model object\n",
    "    :param metric: Metric to show: precision, recall, accuracy\n",
    "    \"\"\"\n",
    "    lst = list(np.arange(0.0, 1.0, 0.1))\n",
    "    X = pd.concat([X_train,X_test])\n",
    "    y = pd.concat([y_train,y_test])\n",
    "    score = []\n",
    "    for k in lst:\n",
    "        features = get_features_variance_threshold(X, threshold=k)\n",
    "        \n",
    "        X_train = X_train[features]\n",
    "        X_test = X_test[features]\n",
    "        \n",
    "#         print(\"Variance: {} - features: {}\".format(k,features))\n",
    "\n",
    "        model.fit(X_train, y_train)\n",
    "        mean = cross_val_score(model, X_test, y_test, cv=10).mean()\n",
    "        variance = cross_val_score(model, X_test, y_test, cv=10).var()\n",
    "        score.append([k, mean, variance])\n",
    "#     print(score)\n",
    "    df = pd.DataFrame(score, columns=[\"variance\", \"mean\", \"std\"])\n",
    "    sns.pointplot(df[\"variance\"], df[\"mean\"])\n",
    "    plt.show()\n",
    "    return df.sort_values(\"mean\", ascending=False)[[\"variance\"]].values[0][0]\n",
    "    \n",
    "def full_report(X_train, X_test, y_train, y_test, model):\n",
    "    \"\"\" Prints classification report for model over different feature selection models\n",
    "    :param X_train: A pandas dataFrame with train features\n",
    "    :param X_test: A pandas dataFrame with test features\n",
    "    :param y_train: A pandas dataFrame with train labels\n",
    "    :param y_test: A pandas dataFrame with test labels\n",
    "    :param model: Sklearn object\n",
    "    \"\"\"\n",
    "    X = pd.concat([X_train, X_test])\n",
    "    y = pd.concat([y_train, y_test])\n",
    "    \n",
    "    base = pd.concat([X,y], axis=1)\n",
    "    print(\"\"\"###############################################################\"\"\")\n",
    "    print(\"\"\"Feature Selection: F-SCORE: \\n\"\"\")\n",
    "    k = change_of_evaluation_univariate_feature_selection(X_train, X_test, y_train, y_test,\n",
    "                                                              model, metric=\"accuracy\")\n",
    "    features = get_features_univariate(base, k=k, linear=True)\n",
    "    print(\"Number of features: \", len(features))\n",
    "    print(\"Features: \", features)\n",
    "    generate_classification_report(X_train, X_test, y_train, y_test, r_forest)\n",
    "    print(\"\"\"###############################################################\"\"\")\n",
    "    print(\"\"\"Feature Selection: MUTUAL INFORMATION: \\n\"\"\")\n",
    "\n",
    "    k = change_of_evaluation_univariate_feature_selection(X_train, X_test, y_train, y_test,\n",
    "                                                              model,linear=False, metric=\"accuracy\")\n",
    "    features = get_features_univariate(base, k=k, linear=False)\n",
    "    print(\"Number of features: \", len(features))\n",
    "    print(\"Features: \", features)\n",
    "    generate_classification_report(X_train, X_test, y_train, y_test, r_forest)\n",
    "    print(\"\"\"###############################################################\"\"\")\n",
    "    print(\"\"\"Feature Selection: Variance Threshold: \\n\"\"\")\n",
    "    v = change_of_evaluation_variance_threshold(X_train, X_test, y_train, y_test,model, metric=\"accuracy\")\n",
    "#     print(v)\n",
    "    features = get_features_variance_threshold(base,threshold=v)\n",
    "    print(\"Number of features: \", len(features))\n",
    "    print(\"Features: \", features)\n",
    "    generate_classification_report(X_train, X_test, y_train, y_test, r_forest)\n",
    "    \n",
    "    print(\"\"\"###############################################################\"\"\")\n",
    "    print(\"\"\"Feature Selection: RFE: \\n\"\"\")\n",
    "    k = change_of_evaluation_rfe(X_train, X_test, y_train, y_test,\n",
    "                                 model,metric='accuracy')\n",
    "    features = get_features_rfe(base,model, k=k)\n",
    "    print(\"Number of features: \", len(features))\n",
    "    print(\"Features: \", features)\n",
    "    generate_classification_report(X_train, X_test, y_train, y_test, model)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def over_sample(X,y):\n",
    "    \"\"\"\n",
    "    Does the hybrid oversampling with SMOTE\n",
    "    :param X: Dataframe with predictor atributes\n",
    "    :param y: Dataframe with labels\n",
    "    :return X_resampled, y_resampled:\n",
    "    \"\"\"\n",
    "    sm = SMOTE(random_state=42)\n",
    "    X,y = sm.fit_sample(X, y)\n",
    "    print(X.shape, y.shape)\n",
    "    return X,y\n",
    "\n",
    "def train_test_dfs(X,y,test_size=0.2):\n",
    "    \"\"\"\n",
    "    Returns train, test dataframees\n",
    "    :parma X: data frame with predictor atributes \n",
    "    :parma y: data frame with labels \n",
    "    :parma test_size: fraction of data set to be assigned to the test set\n",
    "    :return X_train, X_test, y_train, y_test:\n",
    "    \"\"\"\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size)\n",
    "    X_train = pd.DataFrame(X_train, columns=X.columns)\n",
    "    X_test = pd.DataFrame(X_test, columns=X.columns)\n",
    "    y_train = pd.DataFrame(y_train, columns=y.columns)\n",
    "    y_test = pd.DataFrame(y_test, columns=y.columns)\n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "def evaluate_model(estimator, X_train, X_test, y_train, y_test): \n",
    "    \"\"\"\n",
    "    Prints accuracy, confusion matrix and classification report\n",
    "    \"\"\"\n",
    "    target_names = ['female', 'male']\n",
    "    estimator.fit(X_train,y_train)\n",
    "    print(accuracy_score(y_test, estimator.predict(X_test)))\n",
    "    print(confusion_matrix(y_test, estimator.predict(X_test)))\n",
    "    print(classification_report(y_test, estimator.predict(X_test), target_names=target_names))\n",
    "    \n",
    "def get_ac_pr_re_f1(estimator, X,y):\n",
    "    \"\"\"\n",
    "    Returns:\n",
    "    Accuracy, Precission, Recall, F1-Score\n",
    "    \"\"\"\n",
    "    estimator.fit(X_train,y_train)\n",
    "    predict = estimator.predict(X_test)\n",
    "    acc = cross_val_score(estimator, X,y,scoring=\"accuracy\", cv=10).mean()\n",
    "    re = cross_val_score(estimator, X,y,scoring=\"recall\", cv=10).mean()\n",
    "    pr = cross_val_score(estimator, X,y,scoring=\"precision\", cv=10).mean()\n",
    "    f1 = cross_val_score(estimator, X,y,scoring=\"f1\", cv=10).mean()\n",
    "    return acc,pr,re,f1\n",
    "\n",
    "def model_comparison_df(estimator_dict, X,y):\n",
    "    \"\"\"\n",
    "    Returns table with model performance comparisson\n",
    "    \"\"\"\n",
    "    lst = []\n",
    "    for k,v in estimator_dict.items():\n",
    "        acc, pr, re, f1 = get_ac_pr_re_f1(v, X,y)\n",
    "        lst.append([k, acc, pr, re, f1])\n",
    "    return pd.DataFrame(lst, columns=[\"Algorithm\", \"Accuracy\", \"Precision\", \"Recall\", \"F1\"])\n",
    "\n",
    "def print_latex(df, col_format=\"|c|c|c|c|c|\"):\n",
    "    \"\"\" Prints the latex syntax equivalent to the passed dataframe\n",
    "    :param df: Pandas dataframe \n",
    "    :col_format : String indicating the format of columns\n",
    "    \"\"\"\n",
    "    df = df.round(2)\n",
    "    latex = df.to_latex(column_format=col_format, index=False).replace('toprule',\n",
    "                                            \"hline\").replace('midrule',\n",
    "                                            \"hline\").replace('bottomrule',\n",
    "                                            \"hline\").replace(\"\\\\\\\\\\n\",\n",
    "                                            \"\\\\\\\\\\n\\\\hline\").replace(\"\\hline\\hline\",\"\\hline\")\n",
    "    print(latex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get_ac_pr_re_f1(r_forest,X_over,y_over)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_pickle(\"../data/final_sets/countries/model_large/2016_german\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocess and Normalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = preprocess_data_frame(data, [\"gender\"])\n",
    "data.gender.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = normalize.zscore_wikipedia_entered(data, [\"views\", \"eig_central\",\"in_degree\",\"k_core\",\"out_degree\",\"efficiency\"])\n",
    "print(data.head(1))\n",
    "data.head()\n",
    "\n",
    "sns.distplot(data.eig_central, color=\"grey\")\n",
    "sns.distplot(data[data[\"gender\"]==0].eig_central, color=\"red\")\n",
    "sns.distplot(data[data[\"gender\"]==1].eig_central, color=\"blue\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot(data.views, color=\"grey\")\n",
    "sns.distplot(data[data[\"gender\"]==0].views, color=\"red\")\n",
    "sns.distplot(data[data[\"gender\"]==1].views, color=\"blue\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot(data.in_degree, color=\"grey\")\n",
    "sns.distplot(data[data[\"gender\"]==0].in_degree, color=\"red\")\n",
    "sns.distplot(data[data[\"gender\"]==1].in_degree, color=\"blue\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot(data[\"distance_birth\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = normalize.normalize(data, [\"eig_central\",\"in_degree\",\"k_core\",\"out_degree\",\"efficiency\",\"views\"])\n",
    "\n",
    "sns.distplot(data.views, color=\"grey\")\n",
    "sns.distplot(data[data[\"gender\"]==0].views, color=\"red\")\n",
    "sns.distplot(data[data[\"gender\"]==1].views, color=\"blue\")\n",
    "#data[\"views\"].hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot(data.eig_central, color=\"grey\")\n",
    "sns.distplot(data[data[\"gender\"]==0].eig_central, color=\"red\")\n",
    "sns.distplot(data[data[\"gender\"]==1].eig_central, color=\"blue\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot(data.in_degree, color=\"grey\")\n",
    "sns.distplot(data[data[\"gender\"]==0].in_degree, color=\"red\")\n",
    "sns.distplot(data[data[\"gender\"]==1].in_degree, color=\"blue\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.gender.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X,y=split_dataframe(data,\"gender\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "# y_test = list(np.array(y_test.values).flatten())\n",
    "# y_train = list(np.array(y_train.values).flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data.boxplot(column=\"views\", by=\"gender\")\n",
    "\n",
    "# plot boxplot with seaborn\n",
    "bplot=sns.boxplot(y='views', x='gender', \n",
    "                 data=data, \n",
    "                 width=0.5,\n",
    "                 palette=\"colorblind\")\n",
    " \n",
    "# add swarmplot\n",
    "bplot=sns.swarmplot(y='views', x='gender',\n",
    "              data=data, \n",
    "              color='black',\n",
    "              alpha=0.75)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SVM\n",
    "s_vm = svm.SVC()\n",
    "# Logistic regression\n",
    "log_reg = LogisticRegression()\n",
    "# Decision Tree\n",
    "d_tree = tree.DecisionTreeClassifier()\n",
    "# Random Forest\n",
    "r_forest = RandomForestClassifier()\n",
    "\n",
    "estimator_dict = {\n",
    "    \"SVM\": s_vm,\n",
    "    \"Logistic Regression\": log_reg,\n",
    "    \"Decision Tree\": d_tree,\n",
    "    \"Random Forest\": r_forest\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Oversampling "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_over, y_over = over_sample(X,y)\n",
    "X_over = pd.DataFrame(X_over, columns=X.columns)\n",
    "y_over = pd.DataFrame(y_over, columns=[\"gender\"])\n",
    "y_over[\"gender\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_dfs(X_over,y_over)\n",
    "\n",
    "print(y_train[\"gender\"].value_counts())\n",
    "print(y_test[\"gender\"].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_train[\"gender\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this was never used!!!\n",
    "\n",
    "#X_train_norm = normalize.zscore_wikipedia_entered(X_train, columns=[\"eig_central\",\"in_degree\",\"k_core\",\"out_degree\",\"views\"])\n",
    "#X_test_norm = normalize.zscore_wikipedia_entered(X_test, columns=[\"eig_central\",\"in_degree\",\"k_core\",\"out_degree\",\"views\"])\n",
    "\n",
    "#X_train_norm = normalize.normalize(X_train_norm, columns=[\"eig_central\",\"in_degree\",\"k_core\",\"out_degree\",\"views\"])\n",
    "#X_test_norm = normalize.normalize(X_test_norm, columns=[\"eig_central\",\"in_degree\",\"k_core\",\"out_degree\",\"views\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_val_score(s_vm,X_over,y_over,scoring='accuracy',cv=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_val_score(log_reg,X_over,y_over,scoring='accuracy',cv=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_model(log_reg, X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Tree "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_val_score(d_tree,X_over,y_over,scoring='accuracy',cv=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_model(d_tree, X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_val_score(r_forest,X_over,y_over,scoring='accuracy',cv=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_model(r_forest, X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = model_comparison_df(estimator_dict, X_over,y_over)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_latex(df, col_format=\"|c|c|c|c|c|\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Undersampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_under, y_under = under_sample(X,y)\n",
    "X_under = pd.DataFrame(X_under, columns=X.columns)\n",
    "y_under = pd.DataFrame(y_under, columns=[\"gender\"])\n",
    "# base = pd.concat([X_resampled, y_resampled], axis=1)\n",
    "# base.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_dfs(X_under,y_under)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_norm = normalize.zscore_wikipedia_entered(X_train, columns=[\"eig_central\",\"in_degree\",\"k_core\",\"out_degree\",\"views\"])\n",
    "X_test_norm = normalize.zscore_wikipedia_entered(X_test, columns=[\"eig_central\",\"in_degree\",\"k_core\",\"out_degree\",\"views\"])\n",
    "\n",
    "#X_train_norm = normalize.normalize(X_train_norm, columns=[\"eig_central\",\"in_degree\",\"k_core\",\"out_degree\",\"views\"])\n",
    "#X_test_norm = normalizenormalize(X_test_norm, columns=[\"eig_central\",\"in_degree\",\"k_core\",\"out_degree\",\"views\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_model(s_vm, X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_model(log_reg, X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Tree "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_model(d_tree, X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_model(r_forest, X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = model_comparison_df(estimator_dict, X_under,y_under)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_latex(df, col_format=\"|c|c|c|c|c|\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Countries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "country_dict = {\n",
    "    \"USA\": \"american\",\n",
    "    \"Germany\": \"german\",\n",
    "    \"France\": \"french\",\n",
    "    \"Great Britain\": \"british\",\n",
    "    \"Russia\": \"russian\"\n",
    "} "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_countries_df(country_dict, path, year, estimator):\n",
    "    lst = []\n",
    "    for k,v in country_dict.items():\n",
    "        df = pd.read_pickle(path+str(year)+\"_\"+v)\n",
    "        df = normalize.zscore_wikipedia_entered(df, [\"views\", \"eig_central\",\"in_degree\",\"k_core\",\"out_degree\",\"efficiency\"])\n",
    "        df = normalize.normalize(df, [\"views\", \"eig_central\",\"in_degree\",\"k_core\",\"out_degree\",\"efficiency\"])\n",
    "        df = preprocess_data_frame(df, [\"gender\"])\n",
    "        if \"is_alive_no\" in df.columns and \"distance_delta\" in df.columns and \"other_p\" in df.columns:\n",
    "            df = df.drop([\"is_alive_no\",\"distance_delta\",\"other_p\"],axis=1)\n",
    "        X,y=split_dataframe(df,\"gender\")\n",
    "        X_over, y_over = over_sample(X,y)\n",
    "        X_over = pd.DataFrame(X_over, columns=X.columns)\n",
    "        y_over = pd.DataFrame(y_over, columns=[\"gender\"])\n",
    "        y_over[\"gender\"].value_counts()\n",
    "        acc, pr, re, f1 = get_ac_pr_re_f1(estimator, X_over,y_over)\n",
    "        lst.append([k, acc, pr, re, f1])\n",
    "    return pd.DataFrame(lst, columns=[\"Country\", \"Accuracy\", \"Precision\", \"Recall\", \"F1\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = compare_countries_df(country_dict, \"../data/final_sets/countries/model_large/\", 2016, r_forest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_latex(df, col_format=\"|c|c|c|c|c|\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = compare_countries_df(country_dict, \"../data/final_sets/countries/model/\", 2016, r_forest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_latex(df, col_format=\"|c|c|c|c|c|\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def important_features(X,y, estimator):\n",
    "    estimator.fit(X,y)\n",
    "    df = pd.DataFrame({\n",
    "        \"Feature\": X.columns,\n",
    "        \"Importance\": estimator.feature_importances_\n",
    "    })\n",
    "    return df.sort_values(by=\"Importance\",ascending=False)\n",
    "\n",
    "def compare_features_df(country_dict, path, year, estimator):\n",
    "    lst = []\n",
    "    for k,v in country_dict.items():\n",
    "        df = pd.read_pickle(path+str(year)+\"_\"+v)\n",
    "        df = preprocess_data_frame(df, [\"gender\"])\n",
    "        if \"is_alive_no\" in df.columns and \"distance_delta\" in df.columns and \"other_p\" in df.columns:\n",
    "            df = df.drop([\"is_alive_no\",\"distance_delta\",\"other_p\"],axis=1)\n",
    "        X,y=split_dataframe(df,\"gender\")\n",
    "        X_over, y_over = over_sample(X,y)\n",
    "        X_over = pd.DataFrame(X_over, columns=X.columns)\n",
    "        y_over = pd.DataFrame(y_over, columns=[\"gender\"])\n",
    "        y_over[\"gender\"].value_counts()\n",
    "        lst.append([k,important_features(X_over, y_over, estimator)])\n",
    "    return lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = compare_features_df(country_dict, \"../data/final_sets/countries/model_large/\", 2016, r_forest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x[1][1].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lst = [x[1][1].reset_index(drop=True),x[2][1].reset_index(drop=True),x[3][1].reset_index(drop=True),x[4][1].reset_index(drop=True)]\n",
    "# ,x[3][1],x[4][1]/\n",
    "lst = [x[0][1],x[1][1],x[2][1],x[3][1],x[4][1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat(lst, axis=1)\n",
    "# atrs = df.Feature\n",
    "atrs = df.take([0], axis=1)\n",
    "atrs = atrs.replace({\n",
    "    \"wrt\":\"writer\",\n",
    "    \"sci\":\"scientist\",\n",
    "    \"jur\":\"journalist\",\n",
    "    \"eco\":\"economist\",\n",
    "    \"hst\":\"historian\",\n",
    "    \"spo\":\"athleate\",\n",
    "    \"lyr\":\"lawyer\",\n",
    "    \"phs\":\"physician\",\n",
    "    \"act\":\"actor\",\n",
    "    \"distance_birth\":\"d_birth\",\n",
    "    \"distance_death\":\"d_death\",\n",
    "    \"nationality_num\":\"nationalities\",\n",
    "    \"occupation_num\":\"occupations\",\n",
    "    \"year_interval_1\":\"interval_1\",\n",
    "    \"year_interval_2\":\"interval_2\",\n",
    "    \"year_interval_3\":\"interval_3\",\n",
    "    \"is_alive_unknown\":\"alive_unknown\",\n",
    "    \"is_alive_yes\":\"is_alive\"\n",
    "})\n",
    "us = df.take([1], axis=1)\n",
    "de = df.take([3], axis=1)\n",
    "fr = df.take([5], axis=1)\n",
    "gb = df.take([7], axis=1)\n",
    "ru = df.take([9], axis=1)\n",
    "\n",
    "# df = df.drop(df.columns[2],axis=1)\n",
    "# table = pd.DataFrame({\n",
    "#     \"Attribute\": atrs,\n",
    "#     \"USA\": us,\n",
    "#     \"Germany\": de,\n",
    "#     \"France\":fr,\n",
    "#     \"Great Britain\": gb,\n",
    "#     \"Russia\": ru\n",
    "# })\n",
    "# table[\"Attribute\"] = atrs\n",
    "# table[\"USA\"] = us\n",
    "# table[\"Germany\"] = de\n",
    "# table[\"France\"] = fr\n",
    "# table[\"Great Britain\"] = gb\n",
    "# table[\"Russia\"] = ru\n",
    "table = pd.concat([atrs, us,de,fr,gb,ru],axis=1)\n",
    "table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pd.concat(lst, axis=1).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print_latex(pd.concat(lst, axis=1).head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# us = x[0][1][\"USA\"] = x[0][1].apply(lambda x:x[\"Feature\"]+\" \"+\"({})\".format(round(x[\"Importance\"],3)), axis=1).reset_index(drop=True)\n",
    "# de = x[1][1][\"Germany\"] = x[1][1].apply(lambda x:x[\"Feature\"]+\" \"+\"({})\".format(round(x[\"Importance\"],3)), axis=1).reset_index(drop=True)\n",
    "# fr = x[2][1][\"France\"] = x[2][1].apply(lambda x:x[\"Feature\"]+\" \"+\"({})\".format(round(x[\"Importance\"],3)), axis=1).reset_index(drop=True)\n",
    "# gb = x[3][1][\"Great Britain\"] = x[3][1].apply(lambda x:x[\"Feature\"]+\" \"+\"({})\".format(round(x[\"Importance\"],3)), axis=1).reset_index(drop=True)\n",
    "# ru = x[4][1][\"Russia\"] = x[4][1].apply(lambda x:x[\"Feature\"]+\" \"+\"({})\".format(round(x[\"Importance\"],3)), axis=1).reset_index(drop=True)\n",
    "# lst = [us,de,fr,gb,ru]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# us = x[0][1][\"USA\"] = x[0][1].apply(lambda x:x[\"Feature\"], axis=1).reset_index(drop=True)\n",
    "# de = x[1][1][\"Germany\"] = x[1][1].apply(lambda x:x[\"Feature\"], axis=1).reset_index(drop=True)\n",
    "# fr = x[2][1][\"France\"] = x[2][1].apply(lambda x:x[\"Feature\"], axis=1).reset_index(drop=True)\n",
    "# gb = x[3][1][\"Great Britain\"] = x[3][1].apply(lambda x:x[\"Feature\"], axis=1).reset_index(drop=True)\n",
    "# ru = x[4][1][\"Russia\"] = x[4][1].apply(lambda x:x[\"Feature\"], axis=1).reset_index(drop=True)\n",
    "# lst = [us,de,fr,gb,ru]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pd.concat(lst,axis=1)\n",
    "# df.columns = [\"USA\", \"Germany\", \"France\", \"Great Britain\", \"Russia\"]\n",
    "# print_latex(df.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_latex(table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:Anaconda3]",
   "language": "python",
   "name": "conda-env-Anaconda3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
